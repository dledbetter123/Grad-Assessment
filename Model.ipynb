{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer\n",
    "import string\n",
    "\n",
    "# the character based cnn required in the paper\n",
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, num_chars, char_embed_size, num_filters, kernel_sizes):\n",
    "        super(CharCNN, self).__init__()\n",
    "        self.char_embedding = nn.Embedding(num_chars, char_embed_size)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(char_embed_size, num_filters, kernel_size=k) # 1 dimensional convolution, not 2d like in 691.\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x's shape is (batch, word_len, char_embed_size)\n",
    "        '''\n",
    "        x = self.char_embedding(x)\n",
    "        # print(x)\n",
    "        x = x.transpose(1, 2)  # (batch, char_embed_size, word_len)\n",
    "        x = [F.relu(conv(x)) for conv in self.convs]  # cnn\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x, 1)  # join measurements\n",
    "        return x\n",
    "\n",
    "# full model, using lstm as highway and cnn from previous function.\n",
    "class CharAwareLM(nn.Module):\n",
    "    def __init__(self, num_chars, char_embed_size, num_filters, kernel_sizes,\n",
    "                 word_vocab_size, word_embed_size, hidden_size, num_layers, dropout):\n",
    "        super(CharAwareLM, self).__init__()\n",
    "        self.char_cnn = CharCNN(num_chars, char_embed_size, num_filters, kernel_sizes)\n",
    "        self.word_embedding = nn.Embedding(word_vocab_size, word_embed_size)\n",
    "        self.lstm = nn.LSTM(char_embed_size + word_embed_size, hidden_size, num_layers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(hidden_size, word_vocab_size)\n",
    "\n",
    "    def forward(self, word_input, char_input, hidden):\n",
    "        char_output = self.char_cnn(char_input)  # Char-CNN output\n",
    "        print(char_output.shape)\n",
    "        # If CharCNN output is 2D (batch_size, features), unsqueeze to 3D (batch_size, seq_len, features)\n",
    "        if char_output.dim() == 2:\n",
    "            char_output = char_output.unsqueeze(1)\n",
    "\n",
    "        word_embedding = self.word_embedding(word_input)  # Word embedding output\n",
    "        \n",
    "        print(word_embedding.shape)\n",
    "        print(char_output.shape)\n",
    "\n",
    "        # Ensure char_output's seq_len dimension matches that of word_embedding\n",
    "        if word_embedding.size(1) != char_output.size(1):\n",
    "            # This assumes char_output has seq_len of 1 and needs to be repeated for each word in the sequence\n",
    "            char_output = char_output.repeat(1, word_embedding.size(1), 1)\n",
    "\n",
    "        print(word_embedding.shape)\n",
    "        print(char_output.shape)\n",
    "\n",
    "        combined_emb = torch.cat((word_embedding, char_output), 2)  # combine\n",
    "        lstm_out, hidden = self.lstm(combined_emb, hidden)  # LSTM output\n",
    "        logits = self.decoder(lstm_out)  # Decode to word space\n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 300])\n",
      "torch.Size([1, 4, 768])\n",
      "torch.Size([2, 1, 300])\n",
      "torch.Size([1, 4, 768])\n",
      "torch.Size([2, 4, 300])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 1 but got size 2 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/davidledbetter/NLP-673/GradAssesment/Model.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidledbetter/NLP-673/GradAssesment/Model.ipynb#W2sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Forward pass through the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidledbetter/NLP-673/GradAssesment/Model.ipynb#W2sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/davidledbetter/NLP-673/GradAssesment/Model.ipynb#W2sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     logits, hidden \u001b[39m=\u001b[39m model(word_input, char_input, hidden)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidledbetter/NLP-673/GradAssesment/Model.ipynb#W2sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLogits output from the model:\u001b[39m\u001b[39m\"\u001b[39m, logits)\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/davidledbetter/NLP-673/GradAssesment/Model.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidledbetter/NLP-673/GradAssesment/Model.ipynb#W2sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mprint\u001b[39m(word_embedding\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidledbetter/NLP-673/GradAssesment/Model.ipynb#W2sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mprint\u001b[39m(char_output\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/davidledbetter/NLP-673/GradAssesment/Model.ipynb#W2sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m combined_emb \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((word_embedding, char_output), \u001b[39m2\u001b[39;49m)  \u001b[39m# Combine\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidledbetter/NLP-673/GradAssesment/Model.ipynb#W2sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m lstm_out, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(combined_emb, hidden)  \u001b[39m# LSTM output\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidledbetter/NLP-673/GradAssesment/Model.ipynb#W2sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(lstm_out)  \u001b[39m# Decode to word space\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 1 but got size 2 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "char_dict = {char: idx + 1 for idx, char in enumerate(string.ascii_letters + string.digits + string.punctuation)}\n",
    "char_dict['<pad>'] = 0\n",
    "char_dict['<unk>'] = -1\n",
    "\n",
    "# Prepare model hyperparameters\n",
    "num_chars = len(char_dict)\n",
    "char_embed_size = 50\n",
    "num_filters = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "word_vocab_size = tokenizer.vocab_size\n",
    "word_embed_size = 768  # BERT-base hidden size\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "\n",
    "def simple_tokenize(sentence):\n",
    "    # Include space as a separate token\n",
    "    return [token for token in sentence.split(' ')] + [' ']\n",
    "\n",
    "def char_indices(word, char_dict):\n",
    "    return [char_dict.get(c, char_dict['<unk>']) for c in word] # make unknowns default return\n",
    "\n",
    "def pad_sequences(sequences, maxlen, padding='post'):\n",
    "    # Pads sequences to the same length\n",
    "    num_instances = len(sequences)\n",
    "    x = torch.zeros((num_instances, maxlen), dtype=torch.long)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if len(seq) != 0:\n",
    "            if padding == 'pre':\n",
    "                x[i, -len(seq):] = torch.tensor(seq[:maxlen], dtype=torch.long)\n",
    "            else:\n",
    "                x[i, :len(seq)] = torch.tensor(seq[:maxlen], dtype=torch.long)\n",
    "    return x\n",
    "\n",
    "def sentence_to_char_toks(sentence):\n",
    "    word_tokens = tokenizer.tokenize(sentence)\n",
    "    word_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n",
    "    word_input = tokenizer(sentence, return_tensors=\"pt\")['input_ids']\n",
    "    char_inputs = [char_indices(word, char_dict) for word in word_tokens]\n",
    "    char_input = pad_sequences(char_inputs, maxlen=max(len(word) for word in word_tokens), padding='post')\n",
    "    return word_input, char_input\n",
    "\n",
    "model = CharAwareLM(\n",
    "    num_chars=num_chars, char_embed_size=char_embed_size,\n",
    "    num_filters=num_filters, kernel_sizes=kernel_sizes,\n",
    "    word_vocab_size=word_vocab_size, word_embed_size=word_embed_size,\n",
    "    hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "sentence = \"hello world\"\n",
    "word_input, char_input = sentence_to_char_toks(sentence)\n",
    "\n",
    "hidden = None  # Should be initialized properly if using an LSTM/GRU\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    logits, hidden = model(word_input, char_input, hidden)\n",
    "\n",
    "print(\"Logits output from the model:\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharAwareLM(nn.Module):\n",
    "    def __init__(self, num_chars, char_embed_size, num_filters, kernel_sizes,\n",
    "                 hidden_size, num_layers, dropout):\n",
    "        super(CharAwareLM, self).__init__()\n",
    "        self.char_cnn = CharCNN(num_chars, char_embed_size, num_filters, kernel_sizes)\n",
    "        self.lstm = nn.LSTM(num_filters * len(kernel_sizes), hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.decoder = nn.Linear(hidden_size, num_chars)\n",
    "\n",
    "    def forward(self, char_input, hidden=None):\n",
    "        char_output = self.char_cnn(char_input)\n",
    "        lstm_out, hidden = self.lstm(char_output.unsqueeze(1), hidden)\n",
    "        logits = self.decoder(lstm_out.squeeze(1))\n",
    "        return logits, hidden\n",
    "\n",
    "# Function to convert sentence to character indices\n",
    "def sentence_to_char_indices(sentence, char_dict):\n",
    "    return [char_dict.get(c, char_dict['<unk>']) for c in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharCNN output: tensor([[6.8491e-01, 2.0344e-01, 1.0650e+00, 8.9655e-01, 7.2934e-01, 4.1404e-01,\n",
      "         4.8933e-01, 6.6406e-01, 7.4998e-01, 5.8523e-01, 3.4747e-01, 4.8025e-01,\n",
      "         2.0250e-01, 6.4700e-01, 6.4783e-01, 1.0188e+00, 4.8314e-01, 8.3459e-01,\n",
      "         3.1567e-01, 3.6570e-02, 0.0000e+00, 4.3364e-01, 7.1529e-01, 4.5174e-01,\n",
      "         8.1316e-01, 1.1642e-01, 2.2513e-01, 3.0454e-01, 2.0519e-01, 1.1365e-01,\n",
      "         6.4268e-01, 8.2018e-01, 5.4768e-01, 4.2836e-02, 0.0000e+00, 8.9296e-01,\n",
      "         2.4308e-01, 3.9687e-01, 8.6700e-02, 3.9741e-01, 6.9783e-01, 6.9034e-01,\n",
      "         4.9028e-01, 0.0000e+00, 4.7883e-03, 5.3528e-01, 2.6567e-01, 0.0000e+00,\n",
      "         1.2820e-01, 3.1328e-01, 4.4955e-01, 6.0443e-01, 4.3893e-01, 5.2464e-01,\n",
      "         8.2506e-01, 7.0163e-01, 5.6531e-01, 8.6727e-01, 3.0726e-01, 3.7456e-01,\n",
      "         0.0000e+00, 8.9690e-01, 8.7869e-01, 2.0284e-01, 0.0000e+00, 9.8108e-01,\n",
      "         3.1052e-01, 7.3248e-01, 0.0000e+00, 3.7946e-01, 4.5382e-01, 2.4551e-01,\n",
      "         3.6059e-01, 4.1672e-01, 1.0707e+00, 3.8561e-01, 5.5689e-03, 1.0335e+00,\n",
      "         9.2770e-01, 3.3957e-01, 6.0610e-01, 1.0657e-01, 6.3954e-01, 7.4602e-01,\n",
      "         1.0669e+00, 5.3095e-01, 5.9498e-01, 0.0000e+00, 7.5807e-01, 4.3030e-01,\n",
      "         5.7244e-01, 8.6524e-01, 3.3267e-01, 2.0388e-01, 1.0443e+00, 5.5711e-01,\n",
      "         0.0000e+00, 1.1802e+00, 7.3823e-02, 3.2212e-01, 3.4105e-01, 7.9455e-02,\n",
      "         9.3871e-01, 0.0000e+00, 2.4944e-01, 0.0000e+00, 0.0000e+00, 5.3936e-02,\n",
      "         0.0000e+00, 0.0000e+00, 3.1665e-02, 1.3772e-01, 0.0000e+00, 2.1672e-01,\n",
      "         0.0000e+00, 6.7928e-01, 4.0902e-01, 4.9576e-01, 0.0000e+00, 5.6869e-02,\n",
      "         7.7175e-01, 5.7241e-01, 5.4035e-01, 3.7098e-01, 4.3741e-01, 1.0597e+00,\n",
      "         9.0817e-01, 3.8196e-01, 1.4755e-02, 8.3847e-02, 2.4996e-01, 3.0489e-01,\n",
      "         0.0000e+00, 1.7647e-01, 8.1717e-01, 0.0000e+00, 1.2565e-03, 4.1023e-01,\n",
      "         2.5930e-01, 1.7510e-01, 9.1487e-01, 1.6546e+00, 4.8702e-01, 1.0595e+00,\n",
      "         0.0000e+00, 6.5692e-02, 1.7518e-01, 1.9486e-01, 0.0000e+00, 0.0000e+00,\n",
      "         3.3138e-01, 3.7379e-01, 1.0974e+00, 9.4624e-01, 1.0758e+00, 7.6562e-01,\n",
      "         2.1517e-01, 1.0438e+00, 1.3299e+00, 7.4211e-01, 5.9794e-02, 7.5944e-01,\n",
      "         0.0000e+00, 1.3198e-01, 0.0000e+00, 1.3806e-01, 0.0000e+00, 1.0134e+00,\n",
      "         3.6909e-01, 4.1822e-01, 3.2882e-01, 1.4320e+00, 7.8863e-01, 3.1483e-01,\n",
      "         6.4191e-01, 2.6514e-01, 1.3753e-01, 6.2056e-01, 8.2244e-01, 1.1033e+00,\n",
      "         0.0000e+00, 0.0000e+00, 5.4231e-01, 4.1129e-02, 9.3638e-01, 8.0487e-01,\n",
      "         1.6365e-01, 6.1191e-01, 5.0573e-01, 2.7516e-01, 2.6962e-01, 2.7880e-01,\n",
      "         3.8020e-01, 5.8216e-01, 4.7523e-01, 1.7853e-01, 1.3432e-02, 6.4540e-01,\n",
      "         1.2673e-01, 5.3282e-01, 1.0719e-01, 2.8675e-01, 6.3425e-01, 3.0672e-01,\n",
      "         3.2648e-02, 0.0000e+00, 2.0317e-01, 1.5767e-02, 5.3144e-01, 0.0000e+00,\n",
      "         0.0000e+00, 8.4722e-02, 3.3045e-01, 9.3205e-01, 0.0000e+00, 1.4091e+00,\n",
      "         0.0000e+00, 2.1458e-03, 6.6905e-02, 1.0591e-02, 1.7653e-01, 2.2692e-01,\n",
      "         2.7038e-02, 2.4923e-01, 3.0591e-01, 3.1967e-01, 5.5717e-01, 0.0000e+00,\n",
      "         0.0000e+00, 2.3476e-01, 5.0771e-01, 0.0000e+00, 2.2758e-01, 4.8702e-02,\n",
      "         6.4154e-01, 8.2087e-01, 0.0000e+00, 2.3660e-01, 8.7498e-01, 7.1718e-02,\n",
      "         0.0000e+00, 2.4005e-01, 2.5826e-01, 2.2852e-01, 6.8400e-01, 1.4123e-01,\n",
      "         0.0000e+00, 0.0000e+00, 6.1352e-01, 0.0000e+00, 2.4139e-01, 0.0000e+00,\n",
      "         7.5770e-01, 0.0000e+00, 5.1958e-01, 0.0000e+00, 2.0314e-01, 5.0573e-01,\n",
      "         9.7794e-02, 0.0000e+00, 4.9455e-01, 2.8123e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 3.2880e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.8371e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0689e+00, 6.6619e-01, 0.0000e+00,\n",
      "         9.5982e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.8401e-01, 0.0000e+00, 1.5921e-01, 3.7753e-01, 2.7224e-01, 5.2565e-01,\n",
      "         1.7382e-02, 0.0000e+00, 8.5531e-01, 0.0000e+00, 0.0000e+00, 3.2605e-01,\n",
      "         3.6077e-01, 0.0000e+00, 3.6008e-01, 0.0000e+00, 2.2574e-01, 7.8476e-01],\n",
      "        [1.3975e-01, 6.0660e-01, 0.0000e+00, 6.7309e-01, 8.8569e-01, 1.8803e-01,\n",
      "         1.3429e+00, 0.0000e+00, 2.7023e-01, 1.1246e+00, 7.4571e-02, 5.9563e-01,\n",
      "         3.4486e-01, 1.2914e+00, 8.4372e-01, 1.2663e+00, 7.0446e-01, 8.3929e-01,\n",
      "         0.0000e+00, 9.8881e-01, 4.1902e-01, 9.9010e-02, 3.0726e-01, 4.8122e-01,\n",
      "         6.5666e-01, 3.3268e-01, 8.8086e-01, 8.1544e-01, 3.7396e-01, 3.3581e-01,\n",
      "         1.5227e-01, 5.3898e-01, 6.1877e-01, 2.4711e-01, 5.1123e-01, 2.0642e-02,\n",
      "         3.4082e-01, 1.0699e+00, 0.0000e+00, 2.1983e-01, 9.0886e-01, 4.7494e-01,\n",
      "         2.2601e-01, 4.2726e-01, 2.2996e-01, 1.0278e+00, 4.7620e-01, 5.0087e-01,\n",
      "         0.0000e+00, 6.3835e-01, 9.1190e-02, 2.7703e-01, 4.6193e-01, 2.2569e-01,\n",
      "         1.3638e-01, 1.6284e-01, 4.0158e-01, 0.0000e+00, 0.0000e+00, 1.1767e+00,\n",
      "         4.2695e-01, 0.0000e+00, 3.8846e-01, 4.8569e-01, 7.6968e-01, 5.6436e-01,\n",
      "         0.0000e+00, 0.0000e+00, 4.0929e-01, 3.7565e-01, 0.0000e+00, 6.6890e-01,\n",
      "         3.8520e-01, 1.9146e-01, 6.0933e-01, 0.0000e+00, 8.6902e-02, 1.1853e+00,\n",
      "         0.0000e+00, 2.1031e-01, 5.8554e-01, 2.7858e-01, 4.4326e-01, 4.4708e-01,\n",
      "         5.9605e-01, 3.4036e-01, 9.8628e-01, 6.4712e-01, 8.3187e-01, 4.2055e-01,\n",
      "         1.2323e+00, 0.0000e+00, 5.2324e-01, 0.0000e+00, 9.7378e-01, 7.3245e-01,\n",
      "         1.5195e-01, 3.5381e-01, 0.0000e+00, 9.3534e-02, 6.7973e-01, 1.5171e-01,\n",
      "         5.6815e-01, 0.0000e+00, 0.0000e+00, 8.9863e-02, 0.0000e+00, 4.9926e-01,\n",
      "         4.1297e-02, 3.8878e-01, 2.9688e-01, 1.2299e-02, 0.0000e+00, 3.8331e-01,\n",
      "         4.5448e-01, 4.5134e-01, 1.0940e-01, 3.2309e-01, 1.8628e-02, 8.3607e-01,\n",
      "         0.0000e+00, 1.1276e+00, 5.0858e-01, 9.5533e-01, 6.9111e-01, 1.4309e+00,\n",
      "         8.3269e-01, 3.4992e-01, 3.2625e-01, 1.2079e-01, 3.1378e-01, 1.0222e+00,\n",
      "         8.6275e-01, 1.5369e-01, 2.8356e-01, 4.3388e-01, 8.8783e-01, 1.0750e+00,\n",
      "         4.4466e-01, 0.0000e+00, 3.9305e-01, 1.4226e+00, 4.6556e-01, 0.0000e+00,\n",
      "         1.2776e-01, 5.6063e-01, 7.4375e-01, 1.1748e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.4577e-01, 0.0000e+00, 1.8198e-01, 1.0088e+00, 0.0000e+00, 6.5706e-01,\n",
      "         0.0000e+00, 0.0000e+00, 9.9112e-03, 2.7098e-01, 3.4335e-01, 5.1044e-01,\n",
      "         0.0000e+00, 0.0000e+00, 5.5632e-01, 5.5434e-01, 0.0000e+00, 3.3940e-01,\n",
      "         3.4745e-01, 0.0000e+00, 5.8574e-01, 5.2962e-01, 5.5642e-01, 6.3591e-01,\n",
      "         9.2000e-01, 0.0000e+00, 5.9832e-01, 2.0454e-01, 0.0000e+00, 1.0804e+00,\n",
      "         4.0147e-01, 0.0000e+00, 7.4345e-01, 0.0000e+00, 4.5674e-01, 3.9759e-01,\n",
      "         8.4871e-01, 4.9498e-01, 3.1240e-01, 0.0000e+00, 1.1984e+00, 7.2189e-01,\n",
      "         4.2189e-01, 3.3564e-01, 0.0000e+00, 0.0000e+00, 1.2551e-02, 1.3526e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.8993e-01, 2.0551e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.1225e-01, 0.0000e+00, 1.9214e-01, 2.3981e-01,\n",
      "         0.0000e+00, 0.0000e+00, 9.5484e-02, 4.3483e-01, 3.9197e-01, 5.2738e-01,\n",
      "         3.9938e-01, 1.8042e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8543e-01,\n",
      "         0.0000e+00, 4.2365e-01, 0.0000e+00, 2.5933e-01, 2.9513e-01, 7.1748e-01,\n",
      "         0.0000e+00, 3.5901e-01, 7.1250e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.4071e-01, 5.5988e-01, 4.9490e-01, 4.2588e-01, 6.0418e-01, 0.0000e+00,\n",
      "         8.2021e-02, 8.3011e-01, 3.0850e-01, 0.0000e+00, 0.0000e+00, 2.4365e-01,\n",
      "         1.7161e-02, 1.9899e-01, 4.5056e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 9.7668e-02, 0.0000e+00, 0.0000e+00, 1.2675e+00, 0.0000e+00,\n",
      "         2.9206e-01, 2.3746e-01, 0.0000e+00, 0.0000e+00, 1.2696e+00, 0.0000e+00,\n",
      "         8.8877e-01, 0.0000e+00, 0.0000e+00, 7.5892e-01, 0.0000e+00, 9.9595e-02,\n",
      "         0.0000e+00, 0.0000e+00, 6.4864e-01, 4.4058e-01, 3.6800e-01, 0.0000e+00,\n",
      "         1.4081e-01, 6.5201e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4575e-01, 6.0378e-02, 1.7539e-02,\n",
      "         5.4024e-01, 0.0000e+00, 2.9102e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.4357e-01, 0.0000e+00, 1.0093e+00, 0.0000e+00, 0.0000e+00, 1.2703e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Create a CharCNN instance\n",
    "char_cnn = CharCNN(num_chars=num_chars, char_embed_size=char_embed_size, num_filters=num_filters, kernel_sizes=kernel_sizes)\n",
    "\n",
    "# Function to convert sentence to character indices\n",
    "def sentence_to_char_input(sentence, char_dict):\n",
    "    char_seqs = []\n",
    "    for word in sentence.split():\n",
    "        char_seq = char_indices(word, char_dict)\n",
    "        char_seqs.append(char_seq)\n",
    "    char_input = pad_sequences(char_seqs, maxlen=max(len(word) for word in sentence.split()), padding='post')\n",
    "    return char_input\n",
    "\n",
    "# Prepare a sample sentence\n",
    "test_sentence = \"Hello World\"\n",
    "char_input = sentence_to_char_input(test_sentence, char_dict)\n",
    "\n",
    "# Add an extra dimension for batch_size since CharCNN expects a batch of words\n",
    "# char_input = char_input.unsqueeze(0) # would also work here if it's a single sequence\n",
    "# char_input = char_input[None, :]\n",
    "\n",
    "# Test the CharCNN\n",
    "with torch.no_grad():\n",
    "    char_cnn_output = char_cnn(char_input)\n",
    "\n",
    "print(\"CharCNN output:\", char_cnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
